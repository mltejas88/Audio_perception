{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c019eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mltej\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "C:\\Users\\mltej\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py:720: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing Audio....\n",
      "üì¢ Transcribing...\n",
      "üìù Recognized Speech: ton right ton ight\n",
      "üîç Predicting Intent...\n",
      "‚úÖ Intent Detected: MOVE_FORWARD\n",
      "Command: MOVE_FORWARD\n",
      "Robot moving forward...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import whisper\n",
    "import pyaudio\n",
    "import wave\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "from rapidfuzz import process\n",
    "\n",
    "# ---------- Audio Recording ----------\n",
    "def record_audio(filename=\"command.wav\", duration=4, rate=16000):\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=rate, input=True, frames_per_buffer=1024)\n",
    "    frames = [stream.read(1024) for _ in range(0, int(rate / 1024 * duration))]\n",
    "    stream.stop_stream(); stream.close(); p.terminate()\n",
    "\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1); wf.setsampwidth(p.get_sample_size(pyaudio.paInt16)); wf.setframerate(rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "# def record_audio(filename=\"command.wav\", duration=4, rate=16000, device_index=1):\n",
    "#     try:\n",
    "#         FORMAT = pyaudio.paInt16\n",
    "#         CHANNELS = 1\n",
    "#         CHUNK = 1024\n",
    "\n",
    "#         audio = pyaudio.PyAudio()\n",
    "\n",
    "#         print(f\"üéôÔ∏è Recording from device index {device_index}...\")\n",
    "\n",
    "#         stream = audio.open(format=FORMAT,\n",
    "#                             channels=CHANNELS,\n",
    "#                             rate=rate,\n",
    "#                             input=True,\n",
    "#                             input_device_index=device_index,\n",
    "#                             frames_per_buffer=CHUNK)\n",
    "\n",
    "#         frames = []\n",
    "#         for _ in range(int(rate / CHUNK * duration)):\n",
    "#             data = stream.read(CHUNK)\n",
    "#             frames.append(data)\n",
    "\n",
    "#         print(\"‚úÖ Done recording.\")\n",
    "\n",
    "#         stream.stop_stream()\n",
    "#         stream.close()\n",
    "#         audio.terminate()\n",
    "\n",
    "#         with wave.open(filename, 'wb') as wf:\n",
    "#             wf.setnchannels(CHANNELS)\n",
    "#             wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "#             wf.setframerate(rate)\n",
    "#             wf.writeframes(b''.join(frames))\n",
    "\n",
    "#         return filename\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error: {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Facebook Wav2Vec 2.0 ----------\n",
    "wav2vec_tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec_model.eval()\n",
    "\n",
    "def transcribe(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = waveform.squeeze()\n",
    "\n",
    "    input_values = wav2vec_tokenizer(waveform.numpy(), return_tensors=\"pt\").input_values\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = wav2vec_model(input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = wav2vec_tokenizer.decode(predicted_ids[0])\n",
    "    return transcription.lower()\n",
    "\n",
    "# ---------- BERT Intent Classifier ----------\n",
    "# intent_labels = [\"MOVE_FORWARD\", \"TURN_LEFT\", \"STOP\", \"TURN_RIGHT\"]\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "# model.load_state_dict(torch.load(\"models/intent_model.pt\", map_location=torch.device('cpu')))\n",
    "# model.eval()\n",
    "\n",
    "# def predict_intent(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=32)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)   # use `model` here, not `intent_model`\n",
    "#         predicted = torch.argmax(outputs.logits, dim=1)\n",
    "#     return intent_labels[predicted.item()]\n",
    "\n",
    "intent_labels = [\"MOVE_FORWARD\", \"TURN_LEFT\", \"STOP\", \"TURN_RIGHT\"]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "model.load_state_dict(torch.load(\"models/intent_model.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "command_phrases = [\"move forward\", \"turn left\", \"stop\", \"turn right\"]\n",
    "\n",
    "def correct_command(text):\n",
    "    best_match, score, _ = process.extractOne(text.lower(), command_phrases)\n",
    "    if score > 70:\n",
    "        return best_match\n",
    "    return text.lower()\n",
    "\n",
    "def predict_intent(text):\n",
    "    corrected_text = correct_command(text)\n",
    "    inputs = tokenizer(corrected_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=32)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted = torch.argmax(outputs.logits, dim=1)\n",
    "    return intent_labels[predicted.item()]\n",
    "\n",
    "\n",
    "\n",
    "def execute_command(cmd):\n",
    "    print(f\"Command: {cmd}\")\n",
    "    if cmd == \"MOVE_FORWARD\":\n",
    "        # publish to ROS or send to motor\n",
    "        print(\"Robot moving forward...\")\n",
    "    elif cmd == \"TURN_LEFT\":\n",
    "        print(\"Turning left\")\n",
    "    elif cmd == \"STOP\":\n",
    "        print(\"Stopping\")\n",
    "    else:\n",
    "        print(\"Unknown action\")\n",
    "\n",
    "# ---------- Main Execution ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Capturing Audio....\")\n",
    "    audio_path = record_audio();\n",
    "    if not os.path.exists(audio_path):\n",
    "        print(f\"‚ùå Audio file '{audio_path}' not found.\")\n",
    "    else:\n",
    "        print(\"üì¢ Transcribing...\")\n",
    "        try:\n",
    "            text = transcribe(audio_path)\n",
    "            print(\"üìù Recognized Speech:\", text)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Transcription failed: {e}\")\n",
    "            text = None\n",
    "\n",
    "        if text:\n",
    "            print(\"üîç Predicting Intent...\")\n",
    "            try:\n",
    "                intent = predict_intent(correct_command(text))\n",
    "                print(\"‚úÖ Intent Detected:\", intent)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Intent prediction failed: {e}\")\n",
    "                \n",
    "                \n",
    "    execute_command(intent);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb91b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "for i in range(p.get_device_count()):\n",
    "    info = p.get_device_info_by_index(i)\n",
    "    print(f\"Device {i}: {info['name']}\")\n",
    "\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41558701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "audio_path = r\"E:\\Germany\\Document\\TU_Dortmund\\Projects\\Audio_Perception\\turn_left.wav\"\n",
    "\n",
    "if os.path.exists(audio_path):\n",
    "    print(\"‚úÖ File exists.\")\n",
    "else:\n",
    "    print(\"‚ùå File not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e73573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "\n",
    "try:\n",
    "    with wave.open(audio_path, 'rb') as wf:\n",
    "        print(\"‚úÖ WAV file opened successfully.\")\n",
    "        print(f\"Channels: {wf.getnchannels()}, Rate: {wf.getframerate()}, Duration: {wf.getnframes() / wf.getframerate():.2f}s\")\n",
    "except wave.Error as e:\n",
    "    print(f\"‚ùå Not a valid WAV file: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
